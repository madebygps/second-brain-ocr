# Second Brain OCR - Development Instructions

This file contains comprehensive development guidelines and architectural patterns for the Second Brain OCR project. Follow these instructions when making changes, adding features, or reviewing code.

## 📋 Project Overview

**Second Brain OCR** is a production-ready Python application that automatically processes images and PDFs, extracts text using Azure Document Intelligence, generates embeddings with Azure OpenAI, and indexes them in Azure AI Search for semantic retrieval.

**Project Status:** ✅ **All core modules enhanced and production-ready** (January 2025)

**Key Technologies:**
- Python 3.13 with modern type hints and `uv` package manager
- Azure Document Intelligence (OCR) - Enhanced with retry mechanisms and health checks
- Azure OpenAI (Embeddings) - Enhanced with performance monitoring (98% test coverage)
- Azure AI Search (Vector Search) - Enhanced with comprehensive validation
- File System Monitoring (watchdog) - Enhanced with robust error handling
- State Management (JSON) - Enhanced with backup and recovery
- Multi-provider Webhooks (Discord, Slack, ntfy) - Enhanced with retry logic
- Docker deployment to Synology NAS
- CI/CD with GitHub Actions and Azure Container Registry
- `uv` for fast dependency management and Python tooling

## ✨ Recent Enhancements (January 2025)

All core modules have been systematically reviewed and enhanced with:

### **Universal Improvements Across All Modules:**
- ✅ **Comprehensive Error Handling**: Try-except blocks with specific exception types
- ✅ **Retry Mechanisms**: Exponential backoff for transient failures
- ✅ **Input Validation**: Type checking and value validation for all parameters
- ✅ **Performance Monitoring**: Timing, counters, and statistics tracking
- ✅ **Health Checks**: Dedicated methods for component health validation
- ✅ **Thread Safety**: Locks for shared state in concurrent operations
- ✅ **Type Safety**: Full mypy compliance with proper type hints
- ✅ **Code Quality**: Passes ruff formatting and linting
- ✅ **Comprehensive Logging**: Detailed logging at appropriate levels
- ✅ **Resource Cleanup**: Proper cleanup in finally blocks and context managers

### **Module-Specific Highlights:**
- **config.py**: Centralized configuration with validation and logging setup
- **ocr.py**: 97% test coverage (34 tests), file validation, performance metrics
- **embeddings.py**: 98% test coverage (42 tests), text chunking, health checks
- **indexer.py**: Document management, search capabilities, index statistics
- **watcher.py**: Event-based + polling, retry logic, performance tracking
- **state.py**: Atomic file operations, automatic backups, recovery mechanisms
- **notifier.py**: Multi-provider support (Discord/Slack/ntfy), retry with backoff
- **main.py**: Orchestration with health checks, graceful shutdown, statistics

## 🏗️ Architecture Principles

### 1. **Centralized Configuration**
- **ALL configuration must go through `src/second_brain_ocr/config.py`**
- Never use `os.getenv()` directly in other modules
- All environment variables have validation, defaults, and range checking
- Configuration includes logging, timeouts, processing parameters

```python
# ✅ CORRECT: Use centralized config
from .config import Config
timeout = Config.HTTP_TIMEOUT

# ❌ WRONG: Direct environment access
import os
timeout = int(os.getenv("TIMEOUT", "10"))
```

### 2. **Centralized Logging**
- **ALL modules must use `Config.get_logger(__name__)`**
- Never create loggers directly with `logging.getLogger()`
- Logging is configured centrally with consistent formatting
- Supports console + file logging with rotation

```python
# ✅ CORRECT: Use centralized logger
from .config import Config
logger = Config.get_logger(__name__)

# ❌ WRONG: Direct logger creation
import logging
logger = logging.getLogger(__name__)
```

### 3. **Module Structure**
Each module follows this pattern:
```python
"""Module docstring describing purpose."""

# Standard library imports
from pathlib import Path
from typing import Any

# Third-party imports
from azure.some.service import SomeClient

# Local imports
from .config import Config

# Initialize logger
logger = Config.get_logger(__name__)

class SomeProcessor:
    """Class docstring."""

    def __init__(self, endpoint: str, api_key: str) -> None:
        # Implementation
```

## 📁 Project Structure

```
src/second_brain_ocr/       # Core application code
├── __init__.py              # Package initialization
├── config.py               # ✅ CENTRALIZED configuration and logging (ENHANCED)
├── main.py                 # ✅ Application orchestrator with health checks (ENHANCED)
├── ocr.py                  # ✅ Azure Document Intelligence (97% coverage, 34 tests) (ENHANCED)
├── embeddings.py           # ✅ Azure OpenAI embeddings (98% coverage, 42 tests) (ENHANCED)
├── indexer.py              # ✅ Azure AI Search with retry mechanisms (ENHANCED)
├── state.py                # ✅ State management with backup/recovery (47% coverage) (ENHANCED)
├── watcher.py              # ✅ File monitoring with health checks (ENHANCED)
└── notifier.py             # ✅ Multi-provider webhooks (Discord/Slack/ntfy) (ENHANCED)

tests/                      # Comprehensive test suite
├── test_config.py          # Configuration validation tests
├── test_embeddings.py      # Embedding tests (42 tests, 98% coverage)
├── test_integration.py     # End-to-end pipeline tests
├── test_ocr.py             # OCR tests (34 tests, 97% coverage)
├── test_state.py           # State management tests (8 tests)
└── test_watcher.py         # File watcher tests (5 tests)

scripts/                    # Utility scripts
├── check_index_stats.py    # Monitor Azure AI Search storage usage
├── clear_index.py          # Clear all documents from search index
└── test_search.py          # Test semantic search functionality

.copilot-instructions.md    # 📚 THIS FILE - Development guidelines
README.md                   # Project overview and setup instructions
```

## 🔧 Configuration Management

### Environment Variables

**Required Azure Services:**
```bash
AZURE_DOC_INTELLIGENCE_ENDPOINT=https://....cognitiveservices.azure.com/
AZURE_DOC_INTELLIGENCE_KEY=key_here
AZURE_OPENAI_ENDPOINT=https://....openai.azure.com/
AZURE_OPENAI_KEY=key_here
AZURE_SEARCH_ENDPOINT=https://....search.windows.net
AZURE_SEARCH_KEY=key_here
```

**Operational Configuration:**
```bash
WATCH_DIR=/brain-notes                    # Directory to monitor
USE_POLLING=true                          # File detection mode
POLLING_INTERVAL=180                      # Seconds between scans
BATCH_SIZE=10                            # Files per batch
```

**Logging Configuration:**
```bash
LOG_LEVEL=INFO                           # DEBUG, INFO, WARNING, ERROR
LOG_TO_FILE=false                        # Enable file logging
LOG_FILE_PATH=/app/data/app.log          # Log file location
LOG_FILE_MAX_SIZE=10                     # MB before rotation
LOG_FILE_BACKUP_COUNT=3                  # Number of backup files
AZURE_LOG_LEVEL=WARNING                  # Azure SDK verbosity
```

**Performance Tuning:**
```bash
HTTP_TIMEOUT=10                          # Webhook timeout (seconds)
AZURE_TIMEOUT=30                         # Azure service timeout
TEXT_CHUNK_MAX_TOKENS=8000              # Text chunking size
TEXT_CHUNK_OVERLAP=200                   # Chunk overlap tokens
FILE_DETECTION_DELAY=1.0                 # Post-detection delay
```

### Adding New Configuration

When adding new configuration:

1. **Add to `Config` class** with validation:
```python
# In config.py
NEW_SETTING: int = _get_int_in_range("NEW_SETTING", default=100, min_val=10, max_val=1000)
```

2. **Add to `.env.example`** with documentation:
```bash
# New Feature Configuration
# NEW_SETTING=100                        # Description of what it does
```

3. **Add validation** if needed:
```python
# In Config.validate() method
if cls.NEW_SETTING < 50:
    errors.append("NEW_SETTING must be at least 50 for optimal performance")
```

## 🧪 Testing Standards

### Test Categories

1. **Unit Tests** (`test_*.py`):
   - Test individual functions and classes
   - Use mocks for external dependencies
   - Fast execution (<1 second per test)

2. **Integration Tests** (`test_integration.py`):
   - Test full pipeline workflows
   - Mock Azure services but test real interactions
   - Validate error handling and edge cases

3. **Configuration Tests** (`test_config.py`):
   - Validate environment variable parsing
   - Test configuration validation logic
   - Ensure defaults work correctly

### Test Patterns

```python
import pytest
from unittest.mock import MagicMock, patch
from src.second_brain_ocr.module import SomeClass

@pytest.fixture
def mock_azure_client():
    """Reusable mock for Azure services."""
    mock = MagicMock()
    mock.some_method.return_value = "expected_result"
    return mock

def test_functionality_with_valid_input(mock_azure_client):
    """Test description with Given-When-Then structure."""
    # Given
    processor = SomeClass(endpoint="https://test.com", api_key="test-key")

    # When
    result = processor.process_something("valid_input")

    # Then
    assert result == "expected_output"
    mock_azure_client.some_method.assert_called_once()
```

### Running Tests

```bash
# Run all tests with coverage
uv run pytest --cov

# Run specific test file
uv run pytest tests/test_config.py -v

# Run with different log levels
LOG_LEVEL=DEBUG uv run pytest tests/test_integration.py -v -s

# Run tests for specific module
uv run python -m pytest tests/test_watcher.py -v
```

## 🔄 Error Handling Patterns

### Graceful Degradation
```python
def process_file(self, file_path: Path) -> bool:
    """Process a file with comprehensive error handling."""
    try:
        # Attempt processing
        result = self._do_processing(file_path)
        logger.info("Successfully processed: %s", file_path.name)
        return True

    except SpecificServiceError as e:
        logger.error("Service error processing %s: %s", file_path.name, e)
        self.notifier.notify_error(file_path, str(e))
        return False

    except (OSError, ValueError) as e:
        logger.error("File error processing %s: %s", file_path.name, e)
        return False

    except Exception as e:
        logger.exception("Unexpected error processing %s", file_path.name)
        return False
```

### Azure Service Error Handling
```python
from azure.core.exceptions import AzureError, ServiceRequestError

try:
    result = azure_client.process(data)
except ServiceRequestError as e:
    if e.status_code == 429:  # Rate limiting
        logger.warning("Rate limited, will retry: %s", e)
        time.sleep(Config.AZURE_RETRY_DELAY)
    else:
        logger.error("Azure service error: %s", e)
except AzureError as e:
    logger.error("Azure error: %s", e)
```

## 🚀 Performance Guidelines

### File Processing
- Process files in batches (configurable via `BATCH_SIZE`)
- Use state management to avoid reprocessing
- Implement exponential backoff for API rate limits
- Log processing metrics (time, word count, file size)

### Memory Management
- Stream large files instead of loading into memory
- Use generators for large data sets
- Clean up temporary resources in `finally` blocks
- Monitor memory usage in production

### Async Considerations
- Current implementation is synchronous by design (simpler)
- Consider async for high-throughput scenarios
- File I/O is the main bottleneck, not CPU

## 🐳 Docker & Deployment

### Container Structure
```dockerfile
FROM python:3.13-slim

# Install uv package manager
COPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/uv

# Copy source code and configuration
COPY pyproject.toml uv.lock ./
COPY src/ src/

# Install dependencies with uv (faster than pip)
RUN uv sync --frozen --no-cache

# Configure volumes and environment
VOLUME ["/brain-notes", "/app/data"]
ENV PATH="/app/.venv/bin:$PATH"

# Run with proper signal handling
CMD ["uv", "run", "python", "-m", "src.second_brain_ocr.main"]
```

### Volume Management
```yaml
volumes:
  - /host/brain-notes:/brain-notes:ro        # Read-only input
  - /host/app-data:/app/data                 # Persistent state
```

### Health Checks
```bash
# Verify application is running and responsive
docker exec container-name python -c "from src.second_brain_ocr.config import Config; print('OK')"
```

## 🔍 Monitoring & Observability

### Logging Levels
- **DEBUG**: Detailed execution flow, variable values
- **INFO**: Normal operations, file processing, initialization
- **WARNING**: Configuration issues, recoverable errors
- **ERROR**: Processing failures, service errors
- **CRITICAL**: Application-level failures

### Key Metrics to Log
```python
# Processing metrics
logger.info("Processed file: %s (%d words, %.2fs)", filename, word_count, duration)

# Performance metrics
logger.info("Batch complete: %d files in %.1fs", file_count, total_time)

# Error metrics
logger.error("OCR failed: %s (attempt %d/%d)", filename, attempt, max_attempts)
```

### Webhook Notifications
- File processing complete
- Batch processing complete
- Critical errors
- Service health status

## 🔐 Security Considerations

### API Keys
- Never log API keys or sensitive data
- Use environment variables only
- Rotate keys regularly
- Use least-privilege access

### Input Validation
```python
def validate_file_path(file_path: Path) -> bool:
    """Validate file path for security."""
    # Check file extension
    if file_path.suffix.lower() not in Config.SUPPORTED_IMAGE_EXTENSIONS:
        return False

    # Prevent path traversal
    if ".." in str(file_path):
        return False

    # Check file size
    if file_path.stat().st_size > Config.MAX_FILE_SIZE:
        return False

    return True
```

## 📝 Code Style & Quality

### Type Hints
```python
# Use modern type hints
from typing import Any, Dict, List, Optional
from pathlib import Path

def process_files(
    files: list[Path],
    config: dict[str, Any]
) -> list[dict[str, Any]]:
    """Process multiple files and return results."""
```

### Documentation
```python
def complex_function(param1: str, param2: int) -> dict[str, Any]:
    """One-line summary of what the function does.

    Longer description if needed, explaining the purpose,
    behavior, and any important considerations.

    Args:
        param1: Description of the first parameter
        param2: Description of the second parameter

    Returns:
        Dictionary containing the results with keys:
        - 'status': Processing status
        - 'data': Processed data

    Raises:
        ValueError: If param1 is empty
        ProcessingError: If processing fails
    """
```

### Import Organization
```python
# Standard library (alphabetical)
import json
import logging
from pathlib import Path
from typing import Any

# Third-party packages (alphabetical)
import requests
from azure.ai.documentintelligence import DocumentIntelligenceClient

# Local imports (relative, alphabetical)
from .config import Config
from .state import StateManager
```

## � Development Commands with uv

### Code Quality & Testing
```bash
# Install dependencies and sync environment
uv sync

# Run linting
uv run ruff check src tests
uv run ruff format src tests  # Auto-format code

# Type checking
uv run mypy src

# Run tests with coverage
uv run pytest --cov
uv run pytest --cov --cov-report=html  # Generate HTML coverage report

# Run specific tests
uv run pytest tests/test_config.py -v
uv run python -m pytest tests/test_watcher.py -v

# Security scanning
uv run bandit -r src

# Run application locally
uv run python -m src.second_brain_ocr.main

# Interactive Python with project dependencies
uv run python
```

### Package Management
```bash
# Add new dependency
uv add package-name

# Add development dependency
uv add --dev pytest-mock

# Remove dependency
uv remove package-name

# Update all dependencies
uv sync --upgrade

# Show installed packages
uv pip list

# Export requirements (if needed)
uv pip compile pyproject.toml -o requirements.txt
```

## �🚦 CI/CD Pipeline

### Quality Gates
1. **Linting**: `uv run ruff check src tests`
2. **Type Checking**: `uv run mypy src`
3. **Testing**: `uv run pytest --cov`
4. **Security**: `uv run bandit -r src`

### Deployment Flow
```
Push to main → Tests Pass → Build Docker → Push to ACR → Deploy to Synology
```

### Branch Protection
- Require PR reviews
- Require status checks to pass
- No direct pushes to main
- Automatically delete merged branches

## 🛠️ Development Workflow

### Setting Up Development Environment
```bash
# Install uv package manager (if not already installed)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Clone and setup project
git clone <repo>
cd second-brain-ocr

# Install dependencies and create virtual environment
uv sync

# Verify installation
uv run python --version  # Should show Python 3.13.x
uv pip list              # Show installed packages

# Setup pre-commit hooks (if configured)
uv run pre-commit install

# Run initial tests
uv run pytest

# Check code quality
uv run ruff check src
uv run mypy src
```

### Making Changes
1. **Create feature branch**: `git checkout -b feature/description`
2. **Make changes** following these patterns
3. **Write tests** for new functionality
4. **Run quality checks**:
   ```bash
   uv run ruff check src tests    # Linting
   uv run ruff format src tests   # Formatting
   uv run mypy src               # Type checking
   uv run pytest --cov          # Tests with coverage
   ```
5. **Create PR** with description
6. **Review and merge**

### Code Review Checklist
- [ ] Follows centralized config/logging patterns
- [ ] Has appropriate tests
- [ ] Includes error handling
- [ ] Updates documentation if needed
- [ ] No hardcoded values
- [ ] Proper type hints
- [ ] Clear commit messages

## 📚 Additional Resources

### Enhanced Module Features

**All modules now include:**
- `health_check()` method returning health status and metrics
- `get_stats()` method for performance monitoring (where applicable)
- Retry mechanisms with configurable attempts and backoff
- Input validation with detailed error messages
- Comprehensive exception handling
- Thread-safe operations with locks
- Detailed logging at all levels
- Type hints for all methods and functions

**Example Usage:**
```python
# Health check pattern (available in all enhanced modules)
health = component.health_check()
if health.get("is_healthy"):
    logger.info("Component healthy")
else:
    logger.warning("Component unhealthy: %s", health.get("error"))

# Statistics pattern (indexer, state, notifier, watcher)
stats = component.get_stats()
logger.info("Success rate: %.1f%%", stats.get("success_rate", 0.0))
```

### Azure Services Documentation
- [Azure Document Intelligence](https://docs.microsoft.com/azure/cognitive-services/document-intelligence/)
- [Azure OpenAI](https://docs.microsoft.com/azure/cognitive-services/openai/)
- [Azure AI Search](https://docs.microsoft.com/azure/search/)

### Development Tools
- [uv Package Manager](https://github.com/astral-sh/uv)
- [Ruff Linter](https://github.com/astral-sh/ruff)
- [MyPy Type Checker](https://mypy.readthedocs.io/)
- [Pytest Testing](https://pytest.org/)

### uv Package Manager Benefits
- **Speed**: 10-100x faster than pip for dependency resolution and installation
- **Reliability**: Lock files ensure reproducible builds across environments
- **Simplicity**: Single tool for dependency management, virtual environments, and script running
- **Modern**: Built-in support for Python 3.13+ and modern packaging standards

### uv Best Practices
```bash
# Always use uv run for executing Python scripts
uv run python script.py          # ✅ Correct
python script.py                  # ❌ May use system Python

# Use uv sync to ensure environment matches lockfile
uv sync                           # ✅ Install exact versions from uv.lock
uv sync --upgrade                 # ✅ Upgrade and update lockfile

# Add dependencies properly
uv add requests                   # ✅ Adds to pyproject.toml and uv.lock
pip install requests              # ❌ Not tracked in project files

# Use uv for all development commands
uv run pytest                    # ✅ Uses project's test environment
uv run mypy src                  # ✅ Uses project's type checker
uv run ruff format .             # ✅ Uses project's formatter
```

### Project Documentation
- **README.md** - Project overview, setup instructions, and getting started guide
- **.copilot-instructions.md** - This file - comprehensive development guidelines

---

## 🎯 Key Mantras

1. **"Configuration through Config class"** - Never bypass centralized config
2. **"Logging through Config.get_logger()"** - Consistent logging everywhere
3. **"uv run for all Python commands"** - Use uv for consistency and speed
4. **"Fail gracefully with user feedback"** - Always handle errors meaningfully
5. **"Test the happy path and edge cases"** - Comprehensive test coverage
6. **"Health checks for all components"** - Monitor and validate service health
7. **"Retry with exponential backoff"** - Handle transient failures gracefully
8. **"Document the why, not just the what"** - Clear intentions in code
9. **"Performance second, correctness first"** - Make it work, then make it fast

## 🎉 Project Status

This project is **production-ready** and designed for deployment on **Synology NAS** with **Azure cloud services**.

**✅ All Core Modules Enhanced (January 2025):**
- Comprehensive error handling and retry mechanisms
- Performance monitoring and health checks
- High test coverage (97-98% for OCR and embeddings)
- Full type safety (mypy compliant)
- Production-grade reliability and observability

**📋 Remaining Work:**
- Create comprehensive test suite for indexer.py (40+ tests recommended)
- Optionally enhance test coverage for state.py, watcher.py, notifier.py, main.py
- Integration testing of complete pipeline
- Performance testing under load

Every change should continue to prioritize **reliability**, **maintainability**, and **operational simplicity**.
